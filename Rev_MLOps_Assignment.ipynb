{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2295f8a-3ebc-4b40-a89c-fbb33b1f37e1",
   "metadata": {},
   "source": [
    "### Install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209d6ced-773c-4b4a-8a19-a123d1c22fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.0-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[K     |████████████                    | 339.8 MB 129.8 MB/s eta 0:00:05    |███████▌                        | 212.0 MB 85.5 MB/s eta 0:00:09"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |███████████████████████████████▌| 892.6 MB 157.7 MB/s eta 0:00:01     |███████████████████▉            | 561.3 MB 128.8 MB/s eta 0:00:03█████████████████▉           | 589.3 MB 6.6 MB/s eta 0:00:48██▉           | 589.5 MB 6.6 MB/s eta 0:00:48:00:48:00:06        | 633.1 MB 51.6 MB/s eta 0:00:06��███████▏     | 740.1 MB 137.2 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 906.5 MB 6.5 kB/s \n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.20.0-cp39-cp39-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 806 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.0 MB 31.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (9.0.1)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[K     |████████████████████████        | 273.0 MB 144.6 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 363.4 MB 4.4 kB/s \n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.8 MB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[K     |██████████████▌                 | 302.3 MB 146.3 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |███████████████████████████████ | 644.2 MB 78.0 MB/s eta 0:00:012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 664.8 MB 2.6 kB/s \n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 3.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from torch) (2022.2.0)\n",
      "Collecting triton==3.1.0\n",
      "  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 209.5 MB 11 kB/s  eta 0:00:0101   |████▏                           | 27.2 MB 26.4 MB/s eta 0:00:07     |█████▌                          | 36.2 MB 26.4 MB/s eta 0:00:07              | 42.1 MB 26.4 MB/s eta 0:00:07     |███████████████████████████▊    | 181.7 MB 8.0 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 127.9 MB 122 kB/s  eta 0:00:01   |▊                               | 3.1 MB 20.9 MB/s eta 0:00:06                    | 5.9 MB 20.9 MB/s eta 0:00:06                      | 6.4 MB 20.9 MB/s eta 0:00:060:06                          | 7.2 MB 20.9 MB/s eta 0:00:06 0:00:06    | 8.6 MB 20.9 MB/s eta 0:00:06▏                             | 8.8 MB 20.9 MB/s eta 0:00:06     |████                            | 16.4 MB 20.9 MB/s eta 0:00:06█████▌                          | 22.1 MB 53.4 MB/s eta 0:00:02     |███████████                     | 44.4 MB 53.4 MB/s eta 0:00:02     |███████████▋                    | 46.6 MB 53.4 MB/s eta 0:00:02     |████████████████▋               | 66.5 MB 53.4 MB/s eta 0:00:02MB/s eta 0:00:01     |██████████████████████████▍     | 105.7 MB 90.4 MB/s eta 0:00:01�█████████████| 127.5 MB 90.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.3 MB 160 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from torch) (2.7.1)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 24.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.6 MB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.1 MB 22.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 27.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 188.7 MB 11 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 207.5 MB 22 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 211.5 MB 25 kB/s s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[K     |████████████████████████████████| 434 kB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting starlette<0.42.0,>=0.40.0\n",
      "  Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 743 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from uvicorn) (8.0.4)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)\n",
      "\u001b[K     |████████████████████████████████| 314 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from wandb) (2.27.1)\n",
      "Collecting platformdirs\n",
      "  Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 427 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 28.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 27.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, typing-extensions, smmap, nvidia-cusparse-cu12, nvidia-cublas-cu12, urllib3, triton, sympy, pydantic-core, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, gitdb, annotated-types, torch, starlette, setproctitle, sentry-sdk, pydantic, platformdirs, h11, gitpython, docker-pycreds, wandb, uvicorn, torchvision, fastapi\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2, torchfrtrace and torchrun are installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script uvicorn is installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script fastapi is installed in '/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed annotated-types-0.7.0 docker-pycreds-0.4.0 fastapi-0.115.3 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 platformdirs-4.3.6 pydantic-2.9.2 pydantic-core-2.23.4 sentry-sdk-2.17.0 setproctitle-1.3.3 smmap-5.0.1 starlette-0.41.2 sympy-1.13.1 torch-2.5.0 torchvision-0.20.0 triton-3.1.0 typing-extensions-4.12.2 urllib3-1.26.20 uvicorn-0.32.0 wandb-0.18.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision fastapi uvicorn wandb pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f301a58e-3f10-4418-bef9-73e7d86cebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.16-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: python-multipart\n",
      "Successfully installed python-multipart-0.0.16\n"
     ]
    }
   ],
   "source": [
    "!pip install python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbdebaa-c481-4efa-bd41-ece76282f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrudra-corteva\u001b[0m (\u001b[33mrudra-corteva-corteva-agriscience\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/f7577cac-6c5b-43f7-96e3-14177b5920ab/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"d173b09b0b60cc1be3702dd4fe7af40f88f9ca8d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8ce1d-324b-48e4-8376-5ef64e56dab7",
   "metadata": {},
   "source": [
    "### 1. Model Training with PyTorch (20 Points)\n",
    "We’ll use PyTorch to define and train a logistic regression model for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f852373-2991-4b00-bf6f-69f6a3fff07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/wandb/run-20241027_173937-svkj9lal\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwinter-dawn-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/svkj9lal\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3138, Accuracy: 95.18%\n",
      "Epoch 2/10, Loss: 0.1462, Accuracy: 97.06%\n",
      "Epoch 3/10, Loss: 0.1136, Accuracy: 97.37%\n",
      "Epoch 4/10, Loss: 0.0951, Accuracy: 97.48%\n",
      "Epoch 5/10, Loss: 0.0846, Accuracy: 97.44%\n",
      "Epoch 6/10, Loss: 0.0780, Accuracy: 97.61%\n",
      "Epoch 7/10, Loss: 0.0696, Accuracy: 97.87%\n",
      "Epoch 8/10, Loss: 0.0649, Accuracy: 97.88%\n",
      "Epoch 9/10, Loss: 0.0608, Accuracy: 97.53%\n",
      "Epoch 10/10, Loss: 0.0573, Accuracy: 97.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   accuracy ▁▆▇▇▇▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: batch_loss ▇▅▆▆▆▅▃▄▂▄▂▂▁▃▁█▃▂▂▂▃▂▄▁▃▂▂▃▃▃▃▁▁▂▂▄▁▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       loss █▃▃▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   accuracy 97.76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: batch_loss 0.01656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       loss 0.05733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwinter-dawn-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/svkj9lal\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241027_173937-svkj9lal/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import wandb\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters to experiment with\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_layers\": [128, 64],\n",
    "    \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"mnist-mlops\", config=config)\n",
    "\n",
    "# Load and preprocess data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LogisticRegression(input_dim=28*28, output_dim=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# Watch the model with wandb\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log the loss to wandb\n",
    "        if batch_idx % 100 == 0:\n",
    "            wandb.log({\"batch_loss\": loss.item(), \"epoch\": epoch})\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": total_loss / len(train_loader), \"accuracy\": accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{config[\"epochs\"]}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec3d5c6-1e52-46c7-8a19-4aa437428065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/wandb/run-20241027_184926-h7vweyhm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdenim-moon-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/h7vweyhm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdenim-moon-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/h7vweyhm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241027_184926-h7vweyhm/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialising a wandb run\n",
    "wandb.init(project=\"mnist-mlops\", config=config)\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Log the model file as an artifact\n",
    "wandb.save(\"model.pth\")\n",
    "\n",
    "# Finish the wandb run when done\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47f3e4-271c-40a4-9435-7e5cec7fb957",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. Model Definition: The LogisticRegression class includes hidden layers and dropout for better regularization.\n",
    "2. Training Process: Uses Adam optimizer and cross-entropy loss. The model trains for 10 epochs, with logs sent to WandB for monitoring.\n",
    "3. Validation: Accuracy is computed on the test set after each epoch, logged to WandB for tracking model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6543eb-8f2c-4eaa-87f9-7c1788e50da2",
   "metadata": {},
   "source": [
    "### 2. Model Monitoring and Logging with Weights and Biases (25 Points)\n",
    "In addition to loss and accuracy, we'll log learning rate, gradient norms, and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "326c2801-4723-4de9-8a3e-9250ced68202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/wandb/run-20241027_174846-his6t6qj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-hill-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/his6t6qj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrose-hill-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops/runs/his6t6qj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rudra-corteva-corteva-agriscience/mnist-mlops\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241027_174846-his6t6qj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Initialize wandb\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_layers\": [128, 64],\n",
    "    \"dropout_rate\": 0.2\n",
    "}\n",
    "wandb.init(project=\"mnist-mlops\", config=config)\n",
    "\n",
    "# Define the function to log predictions\n",
    "def log_predictions(model, data, target, num_samples=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        images = [wandb.Image(data[i].view(28, 28), \n",
    "                              caption=f\"Pred: {predictions[i].item()}, True: {target[i].item()}\") \n",
    "                  for i in range(num_samples)]\n",
    "        wandb.log({\"predictions\": images})\n",
    "\n",
    "# Assume model and test_loader are already defined and trained\n",
    "sample_data, sample_target = next(iter(test_loader))\n",
    "log_predictions(model, sample_data[:10], sample_target[:10])\n",
    "\n",
    "# Finish the wandb run at the end\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb159f1-e8e1-456c-a6bc-5ef46b4949d0",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. Logging Predictions: Uses WandB's image logging feature to visualize a few sample predictions along with their labels.\n",
    "2. Logging Metrics: Detailed logging for each epoch, capturing key metrics like loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6c75-8403-43cf-a969-a5fa94e1dd4c",
   "metadata": {},
   "source": [
    "### 3. Model Deployment with FastAPI (15 Points)\n",
    "The following code sets up a FastAPI application to serve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e7ebc7-0471-485d-8685-8841cfceb6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177/2734835786.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Check if we are running in an environment that already has a running loop (like Jupyter)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 53\u001b[0m         \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/uvicorn/main.py:579\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/uvicorn/server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from PIL import Image\n",
    "import io\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import uvicorn\n",
    "\n",
    "# Define the model with a structure matching the saved state\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Load the trained model (assumes the model has been saved as 'model.pth')\n",
    "model = LogisticRegression(input_dim=28*28, output_dim=10)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# FastAPI setup\n",
    "app = FastAPI()\n",
    "\n",
    "# Preprocessing function\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    image = Image.open(io.BytesIO(await file.read())).convert('L')\n",
    "    image = transform(image).unsqueeze(0).view(-1, 28*28)  # Flatten the image\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return {\"prediction\": predicted.item()}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we are running in an environment that already has a running loop (like Jupyter)\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    else:\n",
    "        import asyncio\n",
    "        asyncio.run(uvicorn.run(app, host=\"0.0.0.0\", port=8000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8660d-f54e-4113-aa9c-c48902ef1b57",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. API Definition: Uses FastAPI to create an endpoint that accepts image uploads and returns predictions.\n",
    "2. Model Inference: Loads a saved PyTorch model, processes the uploaded image, and returns the digit prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065096c-6ef0-4103-85ac-d66b177538ee",
   "metadata": {},
   "source": [
    "### 4. Dockerize the Deployment (10 Points)\n",
    "Create a Dockerfile to containerize the FastAPI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cfac2-2347-4382-8640-e9de6b20f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a Python base image\n",
    "FROM python:3.9\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the necessary files\n",
    "COPY . /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Expose port 8000 for the FastAPI app\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the FastAPI application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96677a6e-7678-442a-96f9-3998efb58be0",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. Base Image: Uses Python 3.9 as the base image.\n",
    "2. Dependencies: Installs the required Python packages.\n",
    "3. Run Command: Runs the FastAPI server using uvicorn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f95e1-407e-451c-b5a2-cd33524b189d",
   "metadata": {},
   "source": [
    "### 5. Submission (5 Points)\n",
    "Create a GitHub repository with the code, Dockerfile, and instructions:\n",
    "\n",
    "1. README.md: Add instructions to run the project.\n",
    "2. GitHub Repository: Include your main.py, Dockerfile, and requirements.txt.\n",
    "3. WandB Report URL: Provide a link to the report for the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bf74d-1ca4-45c1-a24c-37b4ee87da3d",
   "metadata": {},
   "source": [
    "### Bonus: CI/CD Pipeline (10 Points)\n",
    "Here's a simple GitHub Actions workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75291e54-30d3-43d9-bae2-11bc32b487a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Deploy FastAPI App\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "    - name: Run tests\n",
    "      run: pytest\n",
    "    - name: Build Docker image\n",
    "      run: docker build -t mnist-fastapi-app .\n",
    "    - name: Push Docker image to Docker Hub\n",
    "      env:\n",
    "        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}\n",
    "        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}\n",
    "      run: |\n",
    "        echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin\n",
    "        docker tag mnist-fastapi-app $DOCKER_USERNAME/mnist-fastapi-app\n",
    "        docker push $DOCKER_USERNAME/mnist-fastapi-app\n",
    "    - name: Deploy to Heroku\n",
    "      run: |\n",
    "        heroku container:push web --app ${{ secrets.HEROKU_APP_NAME }}\n",
    "        heroku container:release web --app ${{ secrets.HEROKU_APP_NAME }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758aed9-db1e-4d6f-ada3-3314eb1548c3",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. GitHub Actions: Automates the deployment of the FastAPI app using Docker.\n",
    "2. Secrets: Uses GitHub secrets for securely storing DockerHub and Heroku credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234fe7fd-b8b6-4e99-a03d-e11e39ec6699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='wandb.zip' target='_blank'>wandb.zip</a><br>"
      ],
      "text/plain": [
       "/home/f7577cac-6c5b-43f7-96e3-14177b5920ab/wandb.zip"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Compress the wandb folder into a zip file named \"wandb.zip\"\n",
    "shutil.make_archive('wandb', 'zip', 'wandb')\n",
    "\n",
    "# The zip file will be saved as \"wandb.zip\" in the current directory.\n",
    "# If you want to download it in a Jupyter Notebook environment, you can use the following code:\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a link to download the zip file\n",
    "FileLink(r'wandb.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b5bc2-fc77-40c2-b06c-04fd0d4d5785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
